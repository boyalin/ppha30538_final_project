---
title: "30538 Final Project: Reproducible Research"
author: "Zidan Kong, Boya Lin, and Sijie Wu" 
format: html
---

# 1. Cleaning Data

```{python}
# import required packages
import os
import pandas as pd
import altair as alt
alt.renderers.enable('png')
import time
import json
from datetime import date
```

```{python}
# define the base path and load datasets
path = r'/Users/boyalin/Documents/GitHub/ppha30538_ps/ppha30538_final_project/data'

chicago_crash_file = os.path.join(
    path, 'Traffic_Crashes_-_Crashes_20241124.csv')
chicago_people_file = os.path.join(
    path, 'Traffic_Crashes_-_People_20241124.csv')
dc_crash_file = os.path.join(path, 'Crashes_in_DC.csv')

# read Chicago crash data
df_crash = pd.read_csv(chicago_crash_file)
# read Chciago people data
df_people = pd.read_csv(chicago_people_file)
# read DC crash data
df_dc = pd.read_csv(dc_crash_file)

# Note: since the data files are too large, we cannot upload to git
# Drive link for all datasets (including both initial, unmodified dataframes we download and the final versions of the dataframe(s)): https://drive.google.com/drive/folders/1tsSsifhK4LLAWtzXqsEb91gDD3ShDvNx?usp=sharing 
```

```{python}
# keep only wanted column
# filter columns in df_people
df_people = df_people[['PERSON_ID', 'PERSON_TYPE', 'CRASH_RECORD_ID',
                       'VEHICLE_ID', 'CRASH_DATE', 'CITY', 'STATE',
                       'ZIPCODE', 'SEX', 'AGE', 'DRIVERS_LICENSE_STATE', 'DRIVERS_LICENSE_CLASS', 'SAFETY_EQUIPMENT', 'AIRBAG_DEPLOYED']]

# drop unnecessary columns in df_crash
df_crash = df_crash.drop(columns=['LANE_CNT', 'PHOTOS_TAKEN_I',
                                  'STATEMENTS_TAKEN_I', 'DOORING_I', 'WORK_ZONE_I',
                                  'WORK_ZONE_TYPE', 'WORKERS_PRESENT_I'])
```

```{python}
# merge Chicago's crash and people data on CRASH_RECORD_ID and CRASH_DATE
merged_df = pd.merge(df_crash, df_people, on=[
                     'CRASH_RECORD_ID', 'CRASH_DATE'], how='inner')
```

For the purpose of plotting, we'll investigate the traffic situation in 5 years period, start with 2019.  

```{python}
# format Chicago report date column
df_crash['CRASH_DATE'] = pd.to_datetime(
    df_crash['CRASH_DATE'], format="%m/%d/%Y %I:%M:%S %p", errors='coerce')
# select data afte 2019
df_crash_2019 = df_crash[df_crash['CRASH_DATE'] >= '2019-01-01']

# format DC report date column and select data after 2019
df_dc['REPORTDATE'] = pd.to_datetime(df_dc['REPORTDATE'], errors='coerce')
df_dc_2019 = df_dc[df_dc['REPORTDATE'] >= '2019-01-01']

# Referencing chatgpt for the specific time format.
```

```{python}
# format Chicago report date column and select data after 2019
df_people['CRASH_DATE'] = pd.to_datetime(
    df_people['CRASH_DATE'], format="%m/%d/%Y %I:%M:%S %p", errors='coerce')
df_people_2019 = df_people[df_people['CRASH_DATE'] >= '2019-01-01']

# format merge data's report date and select data after 2019
merged_df['CRASH_DATE'] = pd.to_datetime(
    merged_df['CRASH_DATE'], format="%m/%d/%Y %I:%M:%S %p", errors='coerce')
df_merged_after_2019 = merged_df[merged_df['CRASH_DATE'] >= '2019-01-01']
```

```{python}
# save csv to path
merge_path= r'/Users/boyalin/Documents/GitHub/ppha30538_ps/ppha30538_final_project/data/df_merged_after_2019.csv'
df_merged_after_2019.to_csv(merge_path, index=False)
```

# 2. Exploring Data

## a. find the crash pattern in month unit, week unit and hour unit:

```{python}
# value counts to find frequency on month, week and daily basis
month_counts = df_crash_2019['CRASH_MONTH'].value_counts(
).sort_values(ascending=False)
day_counts = df_crash_2019['CRASH_DAY_OF_WEEK'].value_counts(
).sort_values(ascending=False)
hour_counts = df_crash_2019['CRASH_HOUR'].value_counts(
).sort_values(ascending=False)
print(month_counts)
print(day_counts)
print(hour_counts.head(5))
```

The crashes happen more frequently in month May~October.
The crashes happen more frequently in weekend.
The crashes happen more frequently in afternoon from 14~18pm.

## b. find out the sexual indentidy influence

```{python}
# filter dataframe for drivers in crashes
df_drivers = df_merged_2019.loc[df_merged_2019['PERSON_TYPE'] == 'DRIVER']
# get crash counts for each sex catgory as driver
identity_counts = df_drivers['SEX'].value_counts().sort_values(ascending=False)

print(identity_counts)
```

Males have the most counts on crashes as driver.

## c. find out age impact

```{python}
# filter age, delete unrealistic value
df_drivers_age = df_drivers[(df_drivers['AGE'] > 0)
                            & df_drivers['AGE'].notna()]
# categorize age in dataset
bins = [0, 12, 18, 40, 65, float('inf')]
labels = ['Children (<12)', 'Teenagers (12-18)', 'Young Adults (18-40)',
          'Middle Adults (40-65)', 'Old Adults (>65)']
df_drivers_age.loc[:, 'AGE_GROUP'] = pd.cut(
    df_drivers_age['AGE'], bins=bins, labels=labels, right=False)

# Referencing chatgpt for mapping age to age group in the dataset.
```

```{python}
# get crash counts for each age group
age_counts = df_drivers_age['AGE_GROUP'].value_counts(
).sort_values(ascending=False)
print(age_counts)
```

```{python}
# select only age range between 6-12 years old, solve potentially wrong age value 
df_age_12 = df_drivers_age[(df_drivers_age['AGE'] > 6)
                           & (df_drivers_age['AGE'] < 12)]
print(len(df_age_12))
```

Young adults are the type that involves in most of the crashes, we can see from the data that even children can been the driver in a car crash, which reveal the importance of improving our traffic restriction.

# 3. Plotting

```{python}
# prepare data for cross-city comparison
# name each obeservation with city in the dateset
df_crash_2019['city'] = 'Chicago'
df_dc_2019['city'] = 'DC'

# format date column
df_dc_2019['REPORTDATE'] = pd.to_datetime(
    df_dc_2019['REPORTDATE'], errors='coerce')
# rename column for merge
df_dc_2019.rename(columns={'REPORTDATE': 'CRASH_DATE'}, inplace=True)

# transfer date to month level to get monthly crash count for plotting
df_crash_2019['CRASH_MONTH_plot'] = df_crash_2019[
    'CRASH_DATE'].dt.to_period('M')
df_dc_2019['CRASH_MONTH_plot'] = df_dc_2019[
    'CRASH_DATE'].dt.to_period('M')
```

```{python}
# concat Chicago and DC dataset
df_chicago_dc = pd.concat([df_crash_2019, df_dc_2019])
```

```{python}
# save csv to path
concat_path= r'/Users/boyalin/Documents/GitHub/ppha30538_ps/ppha30538_final_project/data/df_chicago_dc.csv'
df_chicago_dc.to_csv(concat_path, index=False)
```

```{python}
print(df_chicago_dc['city'].value_counts())
```

```{python}
# groupby get crash counts for both city on monthly basis
monthly_counts = dc_chi_df.groupby(
    ['city', 'CRASH_MONTH_plot']).size().reset_index(name='Crash_Count')
# format for plot with altair
monthly_counts['CRASH_MONTH_plot'] = monthly_counts['CRASH_MONTH_plot'].dt.to_timestamp()

# create per capita crash counts column
population_chicago = 2638159
population_dc = 681683
population_dict = {'Chicago': population_chicago, 'DC': population_dc}
monthly_counts['Population'] = monthly_counts['city'].map(population_dict)
monthly_counts['Per_Capita_Crash'] = monthly_counts['Crash_Count'] / monthly_counts['Population']
print(monthly_counts)

# Google reference for chicago population and DC population
# Refencing chatgpt for mapping city to population.
```

### Time Seires of Monthly Total Crash Counts for Chicago & DC

```{python}
# plot Chicago and DC monthly crash counts for past five years
time_series_chart = alt.Chart(monthly_counts).mark_line(point=True).encode(
    x=alt.X('CRASH_MONTH_plot:T', title='Month', axis=alt.Axis(
            format='%Y-%b', labelAngle=-45)),
    y=alt.Y('Crash_Count:Q', title='Crash Counts'),
    color=alt.Color('city:N', title='City'),
).properties(
    title='Monthly Crash Counts for Chicago and DC',
    width=800,
    height=400
)

time_series_chart
```

### Time Seires of Monthly Crash Counts per Capita for Chicago & DC
We added this new plot based on the feedback we received from our classmate in the in-class presentation, which would provide a more comparative view.

```{python}
# plot Chicago and DC monthly per cpaita crash counts for past five years
time_capita_chart = alt.Chart(monthly_counts).mark_line(point=True).encode(
    x=alt.X('CRASH_MONTH_plot:T', title='Month', axis=alt.Axis(
            format='%Y-%b', labelAngle=-45)),
    y=alt.Y('Per_Capita_Crash:Q', title='Crash Counts Per Capita'),
    color=alt.Color('city:N', title='City'),
).properties(
    title='Monthly Crash Counts for Chicago and DC Per Capita',
    width=800,
    height=400
)

time_capita_chart
```

###  Damage Levels by Gender

```{python}
# group by gender of drivers in crashes and damage Level
damage_data = df_drivers.groupby(
    ['SEX', 'DAMAGE']).size().reset_index(name='Count')
# draw stacked bar chart for damage level on gender types
stacked_bar_chart = alt.Chart(damage_data).mark_bar().encode(
    x=alt.X('DAMAGE:N', title='Damage Level'),
    y=alt.Y('Count:Q', title='Count'),
    color=alt.Color('SEX:N', title='SEX')
).properties(
    title='Stacked Bar Chart of Damage Levels by Gender',
    width=600,
    height=400
)

stacked_bar_chart
```

###  Crash Counts by Lighting Condition

```{python}
# group by get crash counts for each lighting condition 
light_counts = df_crash_2019['LIGHTING_CONDITION'].value_counts().reset_index()
light_counts.columns = ['LIGHTING_CONDITION', 'Count']
light_counts = light_counts.sort_values(by='Count', ascending=False)
# draw crash counts plot based on lighting condition
bar_chart_light = alt.Chart(light_counts).mark_bar().encode(
    x=alt.X('LIGHTING_CONDITION:N', title='Lightening Condition Type',
            sort=light_counts['LIGHTING_CONDITION'].tolist()),
    y=alt.Y('Count:Q', title='Count'),
    color=alt.Color('LIGHTING_CONDITION:N', legend=None)
).properties(
    title='Crash Counts by Lightening Condition',
    width=800,
    height=400
)

bar_chart_light
```

###  Average Injuries by Lighting Condition

```{python}
# get average injury rate based on lighting condition
grouped_light = df_crash_2019.groupby('LIGHTING_CONDITION')[
    'INJURIES_TOTAL'].mean().reset_index()
grouped_light.rename(columns={'INJURIES_TOTAL': 'Avg_Injuries'}, inplace=True)

# draw graph of average injury rate based on lighting condition
scatter_light = alt.Chart(grouped_light).mark_circle(size=200).encode(
    x=alt.X('LIGHTING_CONDITION:N', title='Lighting Condition'),
    y=alt.Y('Avg_Injuries:Q', title='Average Number of Injuries'),
    color=alt.Color('LIGHTING_CONDITION:N', legend=None)
).properties(
    title='Average Injuries by Lighting Condition',
    width=600,
    height=400
)

scatter_light
```

###  Average Fatal by Lighting Condition

```{python}
# get average fatal rate based on lighting condition
grouped_light_fatal = df_crash_2019.groupby('LIGHTING_CONDITION')[
    'INJURIES_FATAL'].mean().reset_index()
grouped_light_fatal.rename(
    columns={'INJURIES_FATAL': 'Avg_FATAL'}, inplace=True)

# draw average fatal rate based on lighting condition
scatter_light_fatal = alt.Chart(grouped_light_fatal).mark_circle(size=200).encode(
    x=alt.X('LIGHTING_CONDITION:N', title='Lighting Condition'),
    y=alt.Y('Avg_FATAL:Q', title='Average Number of Fatal'),
    color=alt.Color('LIGHTING_CONDITION:N', legend=None)
).properties(
    title='Average Fatal by Lighting Condition',
    width=600,
    height=400
)

scatter_light_fatal
```

###  Average Injuries by Safety Equipment (Seat Belt)

```{python}
# get average injury rate based on safety equipment wear condition
grouped_seatbelt = df_merged_2019.groupby('SAFETY_EQUIPMENT')[
    'INJURIES_TOTAL'].mean().reset_index()
grouped_seatbelt.rename(
    columns={'INJURIES_TOTAL': 'Avg_Injuries'}, inplace=True)

# draw average injury rate based on safety equipment wear condition
scatter_seatbelt = alt.Chart(grouped_seatbelt).mark_circle(size=200).encode(
    x=alt.X('SAFETY_EQUIPMENT:N', title='Safety Equipment'),
    y=alt.Y('Avg_Injuries:Q', title='Average Number of Injuries'),
    color=alt.Color('SAFETY_EQUIPMENT:N', legend=None)
).properties(
    title='Average Injuries by Safety Equipment',
    width=600,
    height=400
)

scatter_seatbelt
```

###  Average Fatal by Safety Equipment

```{python}
# get average fatal rate based on safety equipment wear condition
grouped_belt_fatal = df_merged_2019.groupby('SAFETY_EQUIPMENT')[
    'INJURIES_FATAL'].mean().reset_index()
grouped_belt_fatal.rename(
    columns={'INJURIES_FATAL': 'Avg_FATAL'}, inplace=True)

# draw average fatal rate based on safety equipment wear condition
scatter_belt_fatal_plot = alt.Chart(grouped_belt_fatal).mark_circle(size=200).encode(
    x=alt.X('SAFETY_EQUIPMENT:N', title='Safety Equipment'),
    y=alt.Y('Avg_FATAL:Q', title='Average Number of Fatal'),
    color=alt.Color('SAFETY_EQUIPMENT:N', legend=None)
).properties(
    title='Average Fatal by Safety Equipment',
    width=600,
    height=400
)

scatter_belt_fatal_plot
```

###  Crash Counts by Roadway Surface Condition

```{python}
# get crash counts based on roadway surface condition
road_counts = df_crash_2019['ROADWAY_SURFACE_COND'].value_counts(
).reset_index()
road_counts.columns = ['ROADWAY_SURFACE_COND', 'Count']
road_counts = road_counts.sort_values(by='Count', ascending=False)
# draw crash counts based on roadway surface condition
bar_chart_road = alt.Chart(road_counts).mark_bar().encode(
    x=alt.X('ROADWAY_SURFACE_COND:N', title='Roadway Surface Type',
            sort=road_counts['ROADWAY_SURFACE_COND'].tolist()),
    y=alt.Y('Count:Q', title='Count'),
    color=alt.Color('ROADWAY_SURFACE_COND:N', legend=None)
).properties(
    title='Crash Counts by Roadway Surface Condition',
    width=800,
    height=400
)

bar_chart_road
```

###  Average Injuries by Roadway Surface Condition

```{python}
# get average injury rate based on roadway surface condition
grouped_roadway = df_crash_2019.groupby('ROADWAY_SURFACE_COND')[
    'INJURIES_TOTAL'].mean().reset_index()
grouped_roadway.rename(
    columns={'INJURIES_TOTAL': 'Avg_Injuries'}, inplace=True)

# get average injury rate based on roadway surface condition
grouped_roadway = alt.Chart(grouped_roadway).mark_circle(size=200).encode(
    x=alt.X('ROADWAY_SURFACE_COND:N', title='Roadway Surface Ccondition'),
    y=alt.Y('Avg_Injuries:Q', title='Average Number of Injuries'),
    color=alt.Color('ROADWAY_SURFACE_COND:N', legend=None)
).properties(
    title='Average Injuries by Road Surface Condition',
    width=600,
    height=400
)

grouped_roadway
```

###  Average Fatal by Roadway Surface Condition

```{python}
# get average fatal rate based on roadway surface condition
grouped_road_fatal = df_crash_2019.groupby('ROADWAY_SURFACE_COND')[
    'INJURIES_FATAL'].mean().reset_index()
grouped_road_fatal.rename(
    columns={'INJURIES_FATAL': 'Avg_FATAL'}, inplace=True)
# draw average fatal rate based on roadway surface condition
scatter_road_fatal_plot = alt.Chart(grouped_road_fatal).mark_circle(size=200).encode(
    x=alt.X('ROADWAY_SURFACE_COND:N', title='Roadway Surface Ccondition'),
    y=alt.Y('Avg_FATAL:Q', title='Average Number of Fatal'),
    color=alt.Color('ROADWAY_SURFACE_COND:N', legend=None)
).properties(
    title='Average Fatal by Roadway Surface Ccondition',
    width=600,
    height=400
)

scatter_road_fatal_plot
```

###  Distribution of Primary Type of Causations

```{python}
# get crash counts for each primary crash reason
type_counts = df_crash_2019['PRIM_CONTRIBUTORY_CAUSE'].value_counts(
).reset_index()
type_counts.columns = ['PRIM_CONTRIBUTORY_CAUSE', 'Count']
type_counts = type_counts.sort_values(by='Count', ascending=False)
#draw crash counts for each primary crash reason
bar_chart_type = alt.Chart(type_counts).mark_bar().encode(
    x=alt.X('PRIM_CONTRIBUTORY_CAUSE:N', title='Type',
            sort=type_counts['PRIM_CONTRIBUTORY_CAUSE'].tolist()),
    y=alt.Y('Count:Q', title='Count'),
    color=alt.Color('PRIM_CONTRIBUTORY_CAUSE:N', legend=None)
).properties(
    title='Distribution of Primary Type of Causation',
    width=800,
    height=400
)

bar_chart_type

# Refencing chatgpt for sorting value in graphing.
```

###  Average Injusires by Primary Type of Causation

```{python}
# get average injury rate for each primary crash reason
grouped_type = df_crash_2019.groupby('PRIM_CONTRIBUTORY_CAUSE')[
    'INJURIES_TOTAL'].mean().reset_index()
grouped_type.rename(columns={'INJURIES_TOTAL': 'Avg_Injuries'}, inplace=True)
grouped_type_sorted = grouped_type.sort_values(
    by='Avg_Injuries', ascending=False)

# draw average injury rate for each primary crash reason
grouped_type_plot = alt.Chart(grouped_type_sorted).mark_bar().encode(
    x=alt.X('PRIM_CONTRIBUTORY_CAUSE:N', title='Type',
            sort=grouped_type_sorted['PRIM_CONTRIBUTORY_CAUSE'].tolist()),
    y=alt.Y('Avg_Injuries:Q', title='Average Number of Injuries'),
    color=alt.Color('PRIM_CONTRIBUTORY_CAUSE:N', legend=None)
).properties(
    title='Average Injuries by Primary Type of Causation',
    width=800,
    height=400
)

grouped_type_plot
```


# 4. Data Preparation for Shiny App

```{python}
# keep only necessary columns
merged_for_shiny = merged_df[[
    'CRASH_DATE', 'PRIM_CONTRIBUTORY_CAUSE', 'LATITUDE', 'LONGITUDE']]
# drop rows with Na values in 'CRASH_DATE'
merged_for_shiny = merged_for_shiny.dropna(subset=['CRASH_DATE'])

# convert CRASH_DATE to datetime format
merged_for_shiny['CRASH_DATE'] = pd.to_datetime(
    merged_for_shiny['CRASH_DATE'], errors='coerce')

# extract year from CRASH_DATE
merged_for_shiny['year'] = merged_for_shiny['CRASH_DATE'].dt.year
# ChatGDP reference for AttributeError: Can only use .dt accessor with datetimelike values
```

```{python}
# recategorize the PRIM_CONTRIBUTORY_CAUSE into 10 more general ones to make the ui more user-friendly
# define a mapping dictionary
category_mapping = {
    'Unsafe and Aggressive Driving': [
        'FOLLOWING TOO CLOSELY', 'FAILING TO REDUCE SPEED TO AVOID CRASH',
        'OPERATING VEHICLE IN ERRATIC, RECKLESS, CARELESS, NEGLIGENT OR AGGRESSIVE MANNER',
        'PASSING STOPPED SCHOOL BUS'
    ],

    'Failure to Follow Traffic Signals and Signs': [
        'DISREGARDING TRAFFIC SIGNALS', 'DISREGARDING STOP SIGN', 'DISREGARDING ROAD MARKINGS',
        'DISREGARDING OTHER TRAFFIC SIGNS', 'DISREGARDING YIELD SIGN', 'TURNING RIGHT ON RED'
    ],

    'Incorrect Driving Actions': [
        'IMPROPER BACKING', 'IMPROPER TURNING/NO SIGNAL', 'IMPROPER LANE USAGE',
        'IMPROPER OVERTAKING/PASSING', 'DRIVING ON WRONG SIDE/WRONG WAY'
    ],

    'Failure to Yield and Prioritize Safety': [
        'FAILING TO YIELD RIGHT-OF-WAY'
    ],

    'Environmental and Road Conditions': [
        'WEATHER', 'ROAD ENGINEERING/SURFACE/MARKING DEFECTS', 'ROAD CONSTRUCTION/MAINTENANCE',
        'VISION OBSCURED (SIGNS, TREE LIMBS, BUILDINGS, ETC.)', 'OBSTRUCTED CROSSWALKS', 'RELATED TO BUS STOP'
    ],

    'Distractions': [
        'DISTRACTION - FROM INSIDE VEHICLE', 'DISTRACTION - FROM OUTSIDE VEHICLE',
        'DISTRACTION - OTHER ELECTRONIC DEVICE (NAVIGATION DEVICE, DVD PLAYER, ETC.)', 'TEXTING',
        'CELL PHONE USE OTHER THAN TEXTING'
    ],

    'Impairments': [
        'UNDER THE INFLUENCE OF ALCOHOL/DRUGS (USE WHEN ARREST IS EFFECTED)', 'HAD BEEN DRINKING (USE WHEN ARREST IS NOT MADE)',
        'PHYSICAL CONDITION OF DRIVER', 'DRIVING SKILLS/KNOWLEDGE/EXPERIENCE'
    ],

    'Speeding': [
        'EXCEEDING AUTHORIZED SPEED LIMIT', 'EXCEEDING SAFE SPEED FOR CONDITIONS'
    ],

    'Animals and Nonmotorists': [
        'ANIMAL', 'EVASIVE ACTION DUE TO ANIMAL, OBJECT, NONMOTORIST',
        'BICYCLE ADVANCING LEGALLY ON RED LIGHT', 'MOTORCYCLE ADVANCING LEGALLY ON RED LIGHT'
    ],

    'Vehicle Issues': [
        'EQUIPMENT - VEHICLE CONDITION'
    ],

    'Indeterminate or Not Applicable': [
        'UNABLE TO DETERMINE', 'NOT APPLICABLE'
    ]
}

# flatten the dictionary into a DataFrame
df_mapping = pd.DataFrame([(category, subtype)
                           for category, subtypes in category_mapping.items()
                           for subtype in subtypes],
                          columns=['category', 'subtype'])

# optionally, print the head of resulting DataFrame
print(df_mapping.head())

# ChatGPT reference for make a crosswalk based on given structure
# ChatGPT referene for turning dictionary into a DataFrame
```

```{python}
# merge the mapping DataFrame with the main dataset using 'PRIM_CONTRIBUTORY_CAUSE' and 'subtype'
merged_final_data = merged_for_shiny.merge(df_mapping,
                                           left_on='PRIM_CONTRIBUTORY_CAUSE',
                                           right_on='subtype',
                                           how='left')

# optionally, print the head of final merged dataset
print(merged_final_data.head())
```

```{python}
# bin the latitude and longitude into bins of step size 0.01
merged_final_data['latitude_binned'] = merged_for_shiny['LATITUDE'].round(2)
merged_final_data['longitude_binned'] = merged_for_shiny['LONGITUDE'].round(2)

# clean data by removing rows with NaN or invalid coordinates
merged_final_data_cleaned = merged_final_data.dropna(
    subset=['latitude_binned', 'longitude_binned'])  # drop NaN
merged_final_data_cleaned = merged_final_data_cleaned[(merged_final_data_cleaned['latitude_binned'] != 0) & (
    merged_final_data_cleaned['longitude_binned'] != 0)]  # drop 0s

# display the head of cleaned data
print(merged_final_data_cleaned.head())
```

```{python}
# group by catergory, year, binned longitude, and binned latitude. count obs and sort the values 
aggregated_data = (
    merged_final_data_cleaned.groupby(
        ['category', 'latitude_binned', 'longitude_binned', 'year'])
    .size().reset_index(name='crash_count')
).sort_values('crash_count', ascending=False)

# print number of rows in the aggregated data
print(f'Number of rows in the aggregated DataFrame: {len(aggregated_data)}')
```

```{python}
# save the aggregated data to a CSV file for the Shiny app
output_path = './shiny-app/traffic_crashes_map.csv'
aggregated_data.to_csv(output_path, index=False)
```

```{python}
# load GeoJSON data for visualization
file_path = './data/chicago_neighborhoods.geojson'
with open(file_path) as f:
    chicago_geojson = json.load(f)

geo_data = alt.Data(values=chicago_geojson['features'])
```

```{python}
# create the map (background) using the GeoJSON data
background = alt.Chart(geo_data).mark_geoshape(
    fillOpacity=0, stroke='black', strokeWidth=0.6
).project(type='equirectangular').properties(
    width=320, height=320
)  # alternatively: project(type='identity', reflectY=True)
```

```{python}
# try plotting using Altair in .qmd before coding for Shiny App
# filter and aggregate data for visualization
filtered_data = aggregated_data[
    (aggregated_data['category'] == 'Unsafe and Aggressive Driving') &
    (aggregated_data['year'] >= 2022) &
    (aggregated_data['year'] <= 2024)
]

# set the domain for latitude and longitude based on the data
lat_domain = [filtered_data['latitude_binned'].min() - 0.02,
              filtered_data['latitude_binned'].max() + 0.02]
long_domain = [filtered_data['longitude_binned'].min() - 0.02,
               filtered_data['longitude_binned'].max() + 0.02]

# make a scatter plot and combine it with map background
scatter_plot = alt.Chart(filtered_data).mark_circle().encode(
    x=alt.X('longitude_binned:Q', scale=alt.Scale(
        domain=long_domain), title='Longitude'),
    y=alt.Y('latitude_binned:Q', scale=alt.Scale(
        domain=lat_domain), title='Latitude'),
    size=alt.Size('crash_count:Q', title='Crash Count'),
    color='crash_count:Q',
    tooltip=['latitude_binned', 'longitude_binned', 'crash_count']
).properties(
    title='Traffic Crashes: Unsafe and Aggressive Driving (2022-2024)',
    width=320, height=320
)

final_plot = scatter_plot + background
final_plot.show()
```

# 5. Text Analysis

I aim to extract car crash news articles from the CBS News website and perform text analysis.

Initially, I observe that each article is enclosed in `<h4>` tags. To extract the article titles, hyperlinks, and publication dates, I use the `soup.find_all('h4')` method.

Next, I identify a challenge: the 'LATEST NEWS' section also contains articles and links, but they are unrelated to car crashes. To capture the correct article titles and corresponding URLs, I implement two approaches. I find that each page typically features 25 car crash news articles. Additionally, within the 'LATEST NEWS' section, I notice the presence of the `<h3 class="component__title">Latest News</h3>` tag and the `<div class="component__item-wrapper"></div>` tag. I leverage additional conditions of skipping the specific titles of the latest news. But I find out that the title of lastes news will change over time. So I need to modify my code with the titles changing over time.

Third, in addition to extracting metadata (such as titles, descriptions, and dates) from the main page, I also fetch the full news article content by navigating to the links. I encounter the error `MissingSchema: Invalid URL '/chicago/weather/': No scheme supplied. Perhaps you meant https:///chicago/weather/?`. To resolve this, I revise my code to check whether the extracted URL is relative and, if so, prepend the domain (https://www.cbsnews.com) to it.

Fourth, some articles only contains videos but text, so I add a condition to check for the string "video" in the URL and skip those articles.

Finally, I export the extracted text into a .txt file.

Below is the code for scraping the first page.
```{python}
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin

# File to save the extracted articles
output_file = "articles.txt"

# URL of the main page containing articles
main_page_url = "https://www.cbsnews.com/chicago/tag/car-crash/1/"

# Open the file in write mode
with open(output_file, "w", encoding="utf-8") as file:
    # Fetch and parse the main page
    response = requests.get(main_page_url)
    if response.status_code == 200:
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Counter to track the number of articles processed
        article_count = 0
        
        # Find all <article> elements
        for article in soup.find_all('article', class_='item'):
            # Stop after processing the first 25 articles
            if article_count >= 25:
                break
            
            # Extract the link from the <a> tag
            a_tag = article.find('a', class_='item__anchor')
            link = a_tag['href'] if a_tag and 'href' in a_tag.attrs else None
            
            # Skip if the link contains 'video' (this filters out video links)
            if link and "video" in link:
                continue  # Skip this article if it's a video link
            
            # Ensure the link is an absolute URL
            if link:
                link = urljoin(main_page_url, link)  # Handles both relative and absolute URLs
            
            # Extract the title from the <h4> tag
            title_tag = article.find('h4', class_='item__hed')
            title = title_tag.text.strip() if title_tag else None
            
            # Check if the title contains the specified phrase and break if it does
            if title and "Infant dies after being found unresponsive" in title:
                break
            
            # Extract the description from the <p class="item__dek"> tag
            description_tag = article.find('p', class_='item__dek')
            description = description_tag.text.strip() if description_tag else None
            
            # Extract the date from the <li class="item__date"> tag
            date_tag = article.find('li', class_='item__date')
            date = date_tag.text.strip() if date_tag else None
            
            # Skip articles without required metadata
            if not (title and link and date):
                continue  # Skip this article
            
            # Write metadata to the file
            file.write(f"Title: {title}\n")
            file.write(f"Description: {description}\n")
            file.write(f"Link: {link}\n")
            file.write(f"Date: {date}\n")
            
            # Fetch the full article content
            try:
                article_response = requests.get(link)
                if article_response.status_code == 200:
                    article_soup = BeautifulSoup(article_response.text, 'html.parser')
                    
                    # Extract the main content (usually inside <section class="content__body">)
                    section = article_soup.find('section', class_='content__body')
                    if section:
                        file.write("\nFull Article:\n")
                        for paragraph in section.find_all('p'):
                            file.write(paragraph.text.strip() + "\n")
                        file.write("-" * 80 + "\n\n")
                    else:
                        continue  # Skip if no content found
                else:
                    print(f"Failed to fetch article: {link}")
                    continue  # Skip if article request fails
            except requests.exceptions.RequestException as e:
                print(f"Error fetching article {link}: {e}")
                continue  # Skip if there's an error fetching the article
            
            # Increment the counter
            article_count += 1
    else:
        print(f"Failed to fetch the main page. Status code: {response.status_code}")

print(f"Articles on page 1 have been successfully saved to '{output_file}'")

```

I then apply the web scraping code to the second and third pages, extracting a total of `150` articles in preparation for text analysis.


I update the code to append data from page 2 and 3, by reusing the logic but adjusting the URL for the next page. Below is the code to extract articles and URLs from page 1 to page 3. The text data is stored in the `car_crash_Chicago.txt`.

I ask ChatGPT how to iterate several pages to extract news articles, and make my code consice. It suggests using `urls = []` and `for main_page_url in urls:` as loops.
```{python}
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin

# File to save the extracted articles
output_file = "car_crash_Chicago.txt"

# List of URLs to scrape
urls = [
    "https://www.cbsnews.com/chicago/tag/car-crash/1/",
    "https://www.cbsnews.com/chicago/tag/car-crash/2/",
    "https://www.cbsnews.com/chicago/tag/car-crash/3/",
    "https://www.cbsnews.com/chicago/tag/car-crash/4/"
]

# Open the file in append mode to save articles from multiple pages
with open(output_file, "w", encoding="utf-8") as file:
    for main_page_url in urls:
        # Fetch and parse the main page
        response = requests.get(main_page_url)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Locate the section containing articles under "Car Crash"
            car_crash_section = soup.find('section', attrs={'data-slug': 'car-crash'})
            if not car_crash_section:
                print(f"Car Crash section not found on {main_page_url}")
                continue
            
            # Counter to track the number of articles processed
            article_count = 0
            
            # Find all <article> elements within the relevant container
            for article in car_crash_section.find_all('article', class_='item'):
                # Stop after processing the first 25 articles per page
                if article_count >= 25:
                    break
                
                # Extract the link from the <a> tag
                a_tag = article.find('a', class_='item__anchor')
                link = a_tag['href'] if a_tag and 'href' in a_tag.attrs else None
                
                # Skip if the link contains 'video' (this filters out video links)
                if link and "video" in link:
                    continue  # Skip this article if it's a video link
                
                # Ensure the link is an absolute URL
                if link:
                    link = urljoin(main_page_url, link)  # Handles both relative and absolute URLs
                
                # Extract the title from the <h4> tag
                title_tag = article.find('h4', class_='item__hed')
                title = title_tag.text.strip() if title_tag else None
                
                # Extract the description from the <p class="item__dek"> tag
                description_tag = article.find('p', class_='item__dek')
                description = description_tag.text.strip() if description_tag else None
                
                # Extract the date from the <li class="item__date"> tag
                date_tag = article.find('li', class_='item__date')
                date = date_tag.text.strip() if date_tag else None
                
                # Skip articles without required metadata
                if not (title and link and date):
                    continue  # Skip this article
                
                # Write metadata to the file
                file.write(f"Title: {title}\n")
                file.write(f"Description: {description}\n")
                file.write(f"Link: {link}\n")
                file.write(f"Date: {date}\n")
                
                # Fetch the full article content
                try:
                    article_response = requests.get(link)
                    if article_response.status_code == 200:
                        article_soup = BeautifulSoup(article_response.text, 'html.parser')
                        
                        # Extract the main content (usually inside <section class="content__body">)
                        section = article_soup.find('section', class_='content__body')
                        if section:
                            file.write("\nFull Article:\n")
                            for paragraph in section.find_all('p'):
                                file.write(paragraph.text.strip() + "\n")
                            file.write("-" * 80 + "\n\n")
                        else:
                            continue  # Skip if no content found
                    else:
                        print(f"Failed to fetch article: {link}")
                        continue  # Skip if article request fails
                except requests.exceptions.RequestException as e:
                    print(f"Error fetching article {link}: {e}")
                    continue  # Skip if there's an error fetching the article
                
                # Increment the counter
                article_count += 1
        else:
            print(f"Failed to fetch the main page. Status code: {response.status_code}")

print(f"Articles from all pages under 'Car Crash' section have been successfully saved to '{output_file}'")

```

Now, we will move to the text analysis. I first want to analysis the polarity (negative/ positive) of the sentences.


```{python}
import spacy
nlp = spacy.load("en_core_web_sm")
from spacytextblob.spacytextblob import SpacyTextBlob 
nlp.add_pipe('spacytextblob')

with open("car_crash_Chicago.txt", "r") as file:
    car_crash_Chicago_text = file.read()

doc_crash = nlp(car_crash_Chicago_text)
print(f"Polarity: {doc_crash._.blob.polarity:.2f}")
print(f"Subjectivity: {doc_crash._.blob.subjectivity:.2f}")

import pandas as pd
import altair as alt

crash_sentence_polarities = []
for i, sentence in enumerate(doc_crash.sents):
    polarity = sentence._.blob.polarity
    crash_sentence_polarities.append({"n": i + 1, "crash_polarity": polarity})
df_trump = pd.DataFrame(crash_sentence_polarities)

chart1 = alt.Chart(df_trump).mark_line().encode(
    x = alt.X('n', title = 'Sentence Number'),
    y = alt.Y('crash_polarity', title = "Polarity")
).properties(
    title = "Car Crash in Chicago",
    width = 400,
    height = 100
)

chart1
```

I find out that there is one sentence with the polarity of -1.0, which is very extreme in expressing the negativity. I am curious about what the sentence is, and what tragedy happens. So I select and print what the sentence is, as well as the surrounding 5 sentences.


```{python}
# Variable to store the least polarity and its corresponding sentence
min_polarity = float('inf')
least_polarity_sentence = ""

# Iterate over sentences to find the least polarity
for sentence in doc_crash.sents:
    polarity = sentence._.blob.polarity
    if polarity < min_polarity:
        min_polarity = polarity
        least_polarity_sentence = sentence.text

print(f"The sentence with the least polarity is: \"{least_polarity_sentence.strip()}\"")
print(f"Polarity: {min_polarity:.2f}")

```


```{python}
# Variable to store the least polarity and its index
min_polarity = float('inf')
min_polarity_index = -1

# Store sentences in a list for easier indexing
sentences = list(doc_crash.sents)

# Iterate over sentences to find the least polarity
for i, sentence in enumerate(sentences):
    polarity = sentence._.blob.polarity
    if polarity < min_polarity:
        min_polarity = polarity
        min_polarity_index = i

# Determine the range of sentences to print
start_index = max(0, min_polarity_index - 2)
end_index = min(len(sentences), min_polarity_index + 3)

# Print the sentences and highlight the one with the least polarity
print("Surrounding sentences including the one with the least polarity:")
for i in range(start_index, end_index):
    marker = ">>" if i == min_polarity_index else "  "
    print(f"{marker} Sentence {i + 1}: {sentences[i].text.strip()}")

print(f"\nThe least polarity is: {min_polarity:.2f}")

```

I also want to know which sentence is of the most polarity, and print the surrounding 5 sentences.

I ask ChatGPT how to print surrounding sentence for polarity equal to -1.0. My prompt is in this [link](https://chatgpt.com/share/675272e4-1e60-8002-92b0-aaedad042e6e).
```{python}
# Variable to store the highest polarity and its index
max_polarity = float('-inf')
max_polarity_index = -1

# Store sentences in a list for easier indexing
sentences = list(doc_crash.sents)

# Iterate over sentences to find the one with the highest polarity
for i, sentence in enumerate(sentences):
    polarity = sentence._.blob.polarity
    if polarity > max_polarity:
        max_polarity = polarity
        max_polarity_index = i

# Determine the range of sentences to print
start_index = max(0, max_polarity_index - 2)
end_index = min(len(sentences), max_polarity_index + 3)

# Print the sentences and highlight the one with the highest polarity
print("Surrounding sentences including the one with the highest polarity:")
for i in range(start_index, end_index):
    marker = ">>" if i == max_polarity_index else "  "
    print(f"{marker} Sentence {i + 1}: {sentences[i].text.strip()}")

print(f"\nThe highest polarity is: {max_polarity:.2f}")

```

I now want to perform a word frequency analysis, identify the most frequent word and its count. I ask ChatGPT how to achieve this, it suggests using `most_common` command. 

```{python}
from collections import Counter
import pandas as pd
import altair as alt

# Load the text from the file
with open("car_crash_Chicago.txt", "r") as file:
    car_crash_Chicago_text = file.read()

# Tokenize the text and filter out stop words and punctuation
tokens = [token.text.lower() for token in nlp(car_crash_Chicago_text) if token.is_alpha and not token.is_stop]

# Count word frequencies
word_frequencies = Counter(tokens)

# Identify the most common word and its count
most_common_word, most_common_count = word_frequencies.most_common(1)[0]
print(f"The most frequent word is '{most_common_word}' and it appears {most_common_count} times.")

# Prepare data for visualization (top 10 words by frequency)
top_words = word_frequencies.most_common(30)
df_top_words = pd.DataFrame(top_words, columns=["word", "count"])

# Create a bar chart using Altair
chart = alt.Chart(df_top_words).mark_bar().encode(
    x=alt.X("word", sort="-y", title="Words"),
    y=alt.Y("count", title="Frequency"),
    color=alt.Color("word", legend=None)
).properties(
    title="Top 10 Most Frequent Words",
    width=400,
    height=300
)

chart

```



I also make a text analysis on the car accident in Washington DC. I extract news articles for DC car accident from https://www.fox5dc.com/tag/traffic. It follows the same logic when scraping the news articles for Chicago car crashes.
```{python}
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin

# File to save the extracted articles
output_file = "car_crash_dc.txt"

# List of URLs to scrape
urls = [
    "https://www.fox5dc.com/tag/traffic?page=1",
    "https://www.fox5dc.com/tag/traffic?page=2",
    "https://www.fox5dc.com/tag/traffic?page=3",
    "https://www.fox5dc.com/tag/traffic?page=4",
    "https://www.fox5dc.com/tag/traffic?page=5"
]

# Open the file in write mode to save articles from multiple pages
with open(output_file, "w", encoding="utf-8") as file:
    total_articles_extracted = 0

    for main_page_url in urls:
        # Stop if we have already extracted 100 articles
        if total_articles_extracted >= 100:
            print("Extracted 100 articles, stopping...")
            break
        
        print(f"Processing {main_page_url}")
        
        # Fetch and parse the main page
        response = requests.get(main_page_url)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Locate articles on the main page
            articles = soup.find_all('article', class_='article')  # Finding all articles with 'article' class
            if not articles:
                print(f"No articles found on {main_page_url}")
                continue

            print(f"Found {len(articles)} articles on {main_page_url}")
            
            # Process each article on the page
            for article in articles:
                # Stop after extracting 100 articles
                if total_articles_extracted >= 100:
                    print("Extracted 100 articles, stopping...")
                    break

                # Extract the title and link
                a_tag = article.find('a')
                if not a_tag or 'href' not in a_tag.attrs:
                    continue
                title = a_tag.text.strip()
                link = urljoin(main_page_url, a_tag['href'])
                
                # Extract the description
                description_tag = article.find('p', class_='dek')
                description = description_tag.text.strip() if description_tag else "Description not found"
                
                # Extract the publication time
                time_tag = article.find('time', class_='time')
                if time_tag:
                    publication_time_text = time_tag.get_text(strip=True)  # Extracts visible text
                else:
                    publication_time_text = "Time not found"

                # Fetch the full article content
                try:
                    article_response = requests.get(link)
                    if article_response.status_code == 200:
                        article_soup = BeautifulSoup(article_response.text, 'html.parser')
                        
                        # Extract the main content from <div class="article-body">
                        content_section = article_soup.find('div', class_='article-body')
                        if content_section:
                            content = "\n".join([p.text.strip() for p in content_section.find_all('p')])
                        else:
                            content = "Content not found."
                    else:
                        print(f"Failed to fetch article: {link}")
                        content = "Failed to retrieve article."
                        publication_time_text = "Failed to retrieve time."

                except requests.exceptions.RequestException as e:
                    print(f"Error fetching article {link}: {e}")
                    content = "Error fetching article."
                    publication_time_text = "Error fetching time."

                # Write article details to the file
                file.write(f"Title: {title}\n")
                file.write(f"Link: {link}\n")
                file.write(f"Description: {description}\n")
                file.write(f"Publication Time (Text): {publication_time_text}\n")
                file.write("\nFull Article:\n")
                file.write(content)
                file.write("\n" + "-" * 80 + "\n\n")
                
                # Increment the counter
                total_articles_extracted += 1

                # Check if 100 articles are extracted
                if total_articles_extracted >= 100:
                    print("Extracted 100 articles, stopping...")
                    break

        else:
            print(f"Failed to fetch the main page {main_page_url}. Status code: {response.status_code}")

        # Check if we have reached the desired number of articles
        if total_articles_extracted >= 100:
            break

    print(f"Extracted {total_articles_extracted} articles. Saved to '{output_file}'.")

```

```{python}
with open("car_crash_dc.txt", "r") as file:
    car_crash_dc_text = file.read()

doc_accident = nlp(car_crash_dc_text)
print(f"Polarity: {doc_accident._.blob.polarity:.2f}")
print(f"Subjectivity: {doc_accident._.blob.subjectivity:.2f}")

import pandas as pd
import altair as alt

accident_sentence_polarities = []
for i, sentence in enumerate(doc_accident.sents):
    polarity = sentence._.blob.polarity
    accident_sentence_polarities.append({"n": i + 1, "crash_polarity": polarity})
df_trump = pd.DataFrame(accident_sentence_polarities)

chart1 = alt.Chart(df_trump).mark_line().encode(
    x = alt.X('n', title = 'Sentence Number'),
    y = alt.Y('crash_polarity', title = "Polarity")
).properties(
    title = "Car Crash in DC",
    width = 400,
    height = 100
)

chart1
```